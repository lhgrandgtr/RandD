{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "import configs as c\n",
    "from semantic_kernel.connectors.ai.google.google_ai import GoogleAIChatCompletion\n",
    "\n",
    "from semantic_kernel.connectors.ai.google.google_ai.google_ai_prompt_execution_settings import (\n",
    "    GoogleAIPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents.chat_history import ChatHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a601f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "history = ChatHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96115c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_completion = OllamaChatCompletion(\n",
    "#         ai_model_id='llama3.2'\n",
    "#     )\n",
    "\n",
    "# kernel.add_service(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b454f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.google.google_ai import GoogleAIChatCompletion\n",
    "\n",
    "chat_completion= GoogleAIChatCompletion(\n",
    "    gemini_model_id=\"gemini-2.0-flash\",\n",
    "    api_key=c.GOOGLE_API_KEY,\n",
    ")\n",
    "\n",
    "kernel.add_service(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e0398f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_settings = GoogleAIPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6e1c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the capital of Sri Lanka?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b0f43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "059950de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sri Lanka has two capitals:\n",
      "\n",
      "*   **Sri Jayawardenepura Kotte** is the administrative capital and the location of the parliament.\n",
      "*   **Colombo** is the commercial capital and largest city.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = await chat_completion.get_chat_message_content(\n",
    "            settings=execution_settings,\n",
    "            chat_history=history,\n",
    "            kernel=kernel,\n",
    "        )\n",
    "print(result)\n",
    "\n",
    "history.add_message(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbf252",
   "metadata": {},
   "source": [
    "## Creating a MCP Server from the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9522b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@kernel_function()\n",
    "def echo_function(message: str, extra: str = \"\") -> str:\n",
    "    \"\"\"Echo a message as a function\"\"\"\n",
    "    return f\"Function echo: {message} {extra}\"\n",
    "\n",
    "\n",
    "kernel.add_function(\"echo\", echo_function, \"echo_function\")\n",
    "kernel.add_function(\n",
    "    plugin_name=\"prompt\",\n",
    "    function_name=\"prompt\",\n",
    "    prompt_template_config=PromptTemplateConfig(\n",
    "        name=\"prompt\",\n",
    "        description=\"This is a prompt\",\n",
    "        template=\"Please repeat this: {{$message}} and this: {{$extra}}\",\n",
    "        input_variables=[\n",
    "            InputVariable(\n",
    "                name=\"message\",\n",
    "                description=\"This is the message.\",\n",
    "                is_required=True,\n",
    "                json_schema='{ \"type\": \"string\", \"description\": \"This is the message.\"}',\n",
    "            ),\n",
    "            InputVariable(\n",
    "                name=\"extra\",\n",
    "                description=\"This is extra.\",\n",
    "                default=\"default\",\n",
    "                is_required=False,\n",
    "                json_schema='{ \"type\": \"string\", \"description\": \"This is the message.\"}',\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "server = kernel.as_mcp_server(server_name=\"sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a3755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
